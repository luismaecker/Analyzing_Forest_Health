{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <p style=\"text-align: center;\">Term paper for the Course:</p>\n",
    "#  <p style=\"text-align: center;\">REMOTE SENSING PRODUCTS FOR EARTH SYSTEM RESEARCH </p>\n",
    "\n",
    "\n",
    "\n",
    "The following Jupyter notebook is the result of the work regarding the term paper for the course \"Remote Sensing Products for  Earth System Research\" at Leipzig University WS 23/24, led by Prof. Dr. Jian Peng.\n",
    "\n",
    "The code here represents the full analysis done for this project, making the project easily reproducible. All necessary data is donwloaded within the notebook via google earth engine, except the climatic data from the dwd, as well as the shapefiles for the Nature Reserves Layer and the boundary of germany used for plotting. \n",
    "\n",
    "Regarding the **DWD data** temperature and preciptation data: \n",
    " - It can be downloaded [here](https://opendata.dwd.de/climate_environment/CDC/grids_germany/monthly/)\n",
    " - the data has to be downloaded and unzippedinto monthly folders named: '05_May', \"06_Jun\", \"07_Jul\", \"08_Aug\", \"09_Sep\"\n",
    "\n",
    "Regarding the **shapefiles**:\n",
    "  - The shapefiles for the Nature Reserves Layer can be downloaded [here](https://www.bfn.de/themen/natur-und-mensch/natur/naturschutzgebiete.html)\n",
    "  - The shapefile for the boundary of germany can be downloaded [here](https://www.diva-gis.org/gdata)\n",
    "\n",
    "**Folder structure**: \n",
    "- can be created using the code snippet found in the next cell \n",
    "- only a base path needs to be set \n",
    "- this notebook should then be moved to the process folder\n",
    "\n",
    "\n",
    "**Note:** \n",
    "\n",
    "- some slight modifications to the code regarding file names or folder structure might still be necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "\n",
    "import os\n",
    "\n",
    "# Define base paths for data and output\n",
    "base_path = \"\"  # path where to setup the project\n",
    "\n",
    "\n",
    "# Setup paths\n",
    "data_path = os.path.join(base_path, \"data\")\n",
    "process_path = os.path.join(base_path, \"process\")\n",
    "output_path = os.path.join(base_path, \"output\")\n",
    "ndvi_data_path = os.path.join(data_path, \"ndvi_seasonal\")\n",
    "temp_mean_data_path = os.path.join(data_path, \"dwd\", \"temp_mean\")\n",
    "precip_data_path = os.path.join(data_path, \"dwd\", \"precip\")\n",
    "shapes_data_path = os.path.join(data_path, \"shapes\")\n",
    "corine_data_path = os.path.join(data_path, \"corine\")\n",
    "output_path_mk_ndvi = os.path.join(output_path, \"ndvi_mk\")\n",
    "output_path_climate_m   k = os.path.join(output_path, \"climate_mk\")\n",
    "\n",
    "\n",
    "# List all paths that need to be checked and potentially created\n",
    "paths_to_create = [\n",
    "    ndvi_data_path,\n",
    "    temp_mean_data_path,\n",
    "    precip_data_path,\n",
    "    shapes_data_path,\n",
    "    corine_data_path,\n",
    "    output_path_mk_ndvi,\n",
    "    output_path_climate_mk,\n",
    "    process_path\n",
    "]\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "for path in paths_to_create:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "#  <p style=\"text-align: center;\">Drought stress trend Analysis for temperate Forests</p>\n",
    "\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "Amidst two major vegetation events, namely global greening on a global scale and consecutive drought years in the recent past in Germany, this study investigates the impact of\n",
    "climate change on German forests from 2000 to 2023. Remotely sensed data (MODISNDVI) of the summer month was utilized to assess forest health and dynamics. This study\n",
    "identified significant trends in the NDVI, highlighting variations among forest types and\n",
    "regions. Four regions (Black Forest, Harz Mountains, Rothaar Mountains, Thuringian\n",
    "Slate-Mountains) were identified as especially impacted by a declining NDVI, with a\n",
    "decrease notably pronounced in coniferous forests. The Mann-Kendall test, with time\n",
    "offsets of zero, one, two, and three years, was used in these four regions to examine an\n",
    "association between the NDVI and the climate parameters, temperature and precipitation,\n",
    "however, no significant correlation was identified. This underscores the critical role of\n",
    "local side factors for drought resistance and vulnerability, suggesting a need for fine-scale\n",
    "forest management strategies to mitigate climate change impacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.warp import Resampling\n",
    "from rasterio.mask import mask\n",
    "\n",
    "\n",
    "import pymannkendall as mk\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, LinearSegmentedColormap, ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reoccuring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################\n",
    "# 1. Function to genearate a raster from an array based on a reference raster\n",
    "########################################################\n",
    "# create raster from numpy\n",
    "def create_trend_raster(template_file, np_2d_array, output_file, no_data_value=-9999):\n",
    "    \"\"\"\n",
    "    Updated function to create a raster file from the trend test results or masks,\n",
    "    ensuring the no_data_value is properly handled.\n",
    "\n",
    "    Parameters:\n",
    "    - template_file: Path to an NDVI TIFF file for metadata.\n",
    "    - np_2d_array: 2D NumPy array with the data to write.\n",
    "    - output_file: Path where the output raster will be saved.\n",
    "    - no_data_value: The value to use for indicating no data in the output raster.\n",
    "    \"\"\"\n",
    "    with rasterio.open(template_file) as src:\n",
    "        meta = src.meta.copy()\n",
    "        meta.update(dtype=rasterio.float32, count=1, nodata=no_data_value)\n",
    "\n",
    "        with rasterio.open(output_file, 'w', **meta) as dst:\n",
    "            # If input array is boolean, convert to float32 and use no_data_value for False\n",
    "            if np.issubdtype(np_2d_array.dtype, np.bool_):\n",
    "                np_2d_array = np.where(np_2d_array, 1.0, no_data_value).astype(rasterio.float32)\n",
    "            else:\n",
    "                np_2d_array = np_2d_array.astype(rasterio.float32)\n",
    "                \n",
    "            dst.write(np_2d_array, 1)\n",
    "\n",
    "\n",
    "\n",
    "########################################################\n",
    "# 2. Function to perform Mann-Kendall trend test\n",
    "########################################################\n",
    "\n",
    "def perform_mk_test_progress(data, no_data_value, test):\n",
    "    \"\"\"\n",
    "    Perform the Mann-Kendall trend test on a 3D NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "    - data: A 3D NumPy array with shape (time, height, width).\n",
    "    - no_data_value: The value that indicates no data or missing data.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing two 2D NumPy arrays:\n",
    "      - Trend results (slope of the trend).\n",
    "      - P-values for the trend test.\n",
    "    \"\"\"\n",
    "    # Initialize arrays to store the test results and p-values\n",
    "    trend_result = np.full((data.shape[1], data.shape[2]), no_data_value, dtype=np.float32)\n",
    "    slope_result = np.full((data.shape[1], data.shape[2]), no_data_value, dtype=np.float32)\n",
    "    trend_p_value = np.full((data.shape[1], data.shape[2]), no_data_value, dtype=np.float32)\n",
    "\n",
    "    # Calculate total pixels for progress indicator\n",
    "    total_pixels = data.shape[1] * data.shape[2]\n",
    "    \n",
    "    # Progress bar setup\n",
    "    with tqdm.tqdm(total=total_pixels) as pbar:\n",
    "        # Iterate over each pixel\n",
    "        for i in range(data.shape[1]):\n",
    "            for j in range(data.shape[2]):\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "\n",
    "                # Extract the time series for the current pixel\n",
    "                pixel_series = data[:, i, j]\n",
    "\n",
    "                # Skip the series if it contains the no_data_value\n",
    "                if not np.any(pixel_series == no_data_value):\n",
    "                    if not np.all(np.isnan(pixel_series)):  # Check if the pixel series is not all NaN\n",
    "\n",
    "\n",
    "                        # Mask out no_data_values if necessary before test\n",
    "                        clean_series = pixel_series[pixel_series != no_data_value] if np.any(pixel_series == no_data_value) else pixel_series\n",
    "\n",
    "                        # Perform the test only if the cleaned series is not empty\n",
    "                        if clean_series.size > 0:\n",
    "\n",
    "                            if test == \"seasonal\":\n",
    "                                        result = mk.seasonal_test(clean_series, period=5)\n",
    "                            elif test == \"auto\":  # Changed to elif for proper flow\n",
    "                                result = mk.correlated_seasonal_test(clean_series, period=5)\n",
    "                            else:  # Fixed assignment typo\n",
    "                                result = mk.original_test(clean_series)\n",
    "\n",
    "\n",
    "                            slope_result[i, j] = result.slope\n",
    "                            trend_result[i, j] = 1 if result.trend == \"increasing\" else -1 if result.trend == \"decreasing\" else 0\n",
    "                            trend_p_value[i, j] = result.p\n",
    "\n",
    "    return trend_result, trend_p_value, slope_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Main Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_data_value = -9999\n",
    "# Define CORINE land cover classes of interest\n",
    "forest_classes = [311, 312, 313]\n",
    "\n",
    "\n",
    "# as this script lays in the \"project/process\", we use relative pathing for the main project folder\n",
    "base_path = (r\"..\")\n",
    "\n",
    "# data path\n",
    "data_path = os.path.join(base_path,\"data\")\n",
    "ndvi_data_path = os.path.join(data_path, \"ndvi_seasonal\")\n",
    "\n",
    "# output path\n",
    "output_path = os.path.join(base_path, \"output\")\n",
    "\n",
    "############  Input Paths DWD Data ############\n",
    "\n",
    "temp_mean_data_path = os.path.join(data_path, \"dwd\", \"temp_mean\")\n",
    "precip_data_path = os.path.join(data_path, \"dwd\", \"precip\")\n",
    "\n",
    "\n",
    "############  Input Paths Vector data ############\n",
    "shapes_data_path = os.path.join(data_path, \"shapes\")\n",
    "\n",
    "germany_boundary_shp_path = os.path.join(shapes_data_path, \"germany_v2.shp\")\n",
    "naturparke_shp_path = os.path.join(shapes_data_path, \"Naturparke.shp\")\n",
    "\n",
    "############  Input Paths Corine Data ############\n",
    "corine_data_path = os.path.join(data_path, \"corine\")\n",
    "\n",
    "corine_combined_path = os.path.join(corine_data_path, \"corine_combined.tif\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############  Output Paths Trend Analysis ############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define output file paths\n",
    "output_path_mk_ndvi = os.path.join(output_path, \"ndvi_mk\")\n",
    "output_path_trend_ndvi = os.path.join(output_path_mk_ndvi, \"mk_trend_seasonal.tif\")\n",
    "output_path_slope_ndvi = os.path.join(output_path_mk_ndvi, \"mk_slope_seasonal.tif\")\n",
    "output_path_sig_ndvi = os.path.join(output_path_mk_ndvi, \"mk_sig_seasonal.tif\")\n",
    "\n",
    "# Output path for all mk tests of cliate data\n",
    "output_path_climate_mk = os.path.join(output_path, \"climate_mk\")\n",
    "\n",
    "############  template files (for meta data) for raster output creation ############\n",
    "\n",
    "# get the first file in the ndvi_data_path\n",
    "template_file_ndvi = os.path.join(ndvi_data_path, os.listdir(ndvi_data_path)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using GEE to preprocess and download NDVI and Corine Landcover Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup GEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Packages and initalize GEE\n",
    "\n",
    "import ee\n",
    "import geemap\n",
    "ee.Authenticate(force=False)\n",
    "ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com', project='ee-forest-health')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 NDVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip to germany boundary, Scale to -1 to 1, filter by year and calendar month (months 5-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a FeatureCollection for Germany using the FAO GAUL dataset, filtered for Germany's administrative level 0 (country level)\n",
    "germany = ee.FeatureCollection('FAO/GAUL/2015/level0').filter(ee.Filter.eq('ADM0_NAME', 'Germany'))\n",
    "germany_geometry = germany.geometry()\n",
    "germany_mask = ee.Image().paint(germany, 1).mask()\n",
    "\n",
    "\n",
    "modis_data_ndvi = (\n",
    "    ee.ImageCollection('MODIS/061/MOD13A1')\n",
    "    .select('NDVI')\n",
    "    .filterDate(\"2000-01-01\", \"2024-01-01\")\n",
    "    .filter(ee.Filter.calendarRange(5, 9, 'month'))\n",
    ")\n",
    "\n",
    "\n",
    "################################## Masking with Forest ########################################\n",
    "def apply_clip(image):\n",
    "    return image.clip(germany_geometry)# .clip(forest_geometry)\n",
    "\n",
    "# Function to divide the NDVI band values of an image by 10000\n",
    "def apply_scale(image):\n",
    "    ndvi_scaled = image.multiply(0.0001)\n",
    "    return image.addBands(ndvi_scaled, overwrite=True)\n",
    "\n",
    "ndvi_seasonal = modis_data_ndvi.map(apply_clip).map(apply_scale)\n",
    "                                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geemap.ee_export_image_collection(ndvi_seasonal, out_dir=ndvi_data_path, crs =\"EPSG:25832\", \n",
    "                                  scale=500, region = germany_geometry, file_per_band=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Corine Landcover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to load corine landcover data for a specfic year, use forest classes only, clip to germany boundary, and output at 500 m resolution and with crs \"EPSG:25832\". Then locally it's clipped to the boundary of germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get the landcover data\n",
    "def download_preprocess_corine(landcover_collection, year, gdf):\n",
    "    \n",
    "    landcover_year = landcover_collection.filterDate(str(year-1)+'-01-01', str(year)+'-12-31').first()\n",
    "\n",
    "    zones=ee.Image(0) \\\n",
    "   .where(landcover_year.eq(311), 311) \\\n",
    "   .where(landcover_year.eq(312), 312) \\\n",
    "   .where(landcover_year.eq(313), 313) \n",
    "    \n",
    "    file_path_year = os.path.join(corine_data_path, str(year)+\"_25832_500m.tif\")\n",
    "\n",
    "    geemap.ee_export_image(zones, \n",
    "                           filename = file_path_year, crs =\"EPSG:25832\", \n",
    "                           scale = 500, region = germany_geometry)\n",
    "    \n",
    "\n",
    "    print(\"Cropping to Germany shape\")\n",
    "    \n",
    "    print(gdf.crs)\n",
    "    \n",
    "\n",
    "    # Open the TIFF file\n",
    "    with rasterio.open(file_path_year) as src:\n",
    "        # Crop the image\n",
    "\n",
    "        print(src.crs)\n",
    "        out_image, out_transform = mask(src, gdf.geometry, crop=True)\n",
    "        \n",
    "        # Copy the metadata\n",
    "        out_meta = src.meta.copy()\n",
    "\n",
    "   # Correctly update the metadata, especially dtype and nodata\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": out_image.shape[1],\n",
    "        \"width\": out_image.shape[2],\n",
    "        \"transform\": out_transform,\n",
    "        \"crs\": 'EPSG:25832',\n",
    "        \"dtype\": src.meta['dtype'],  # Ensure dtype matches the original\n",
    "        \"nodata\": src.meta['nodata']  # Preserve original nodata value\n",
    "    })\n",
    "\n",
    "    file_path_corine_cropped = file_path_year.replace(\".tif\",\"_cropped.tif\")\n",
    "    # Write the cropped image to a new file\n",
    "    with rasterio.open(file_path_corine_cropped, 'w', **out_meta) as dest:\n",
    "        dest.write(out_image)\n",
    "\n",
    "    print(\"Remove non-cropped file\")\n",
    "    os.remove(\"file_name_year\")\n",
    "    \n",
    "    return file_path_corine_cropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run function on corine Landcover data for 2000, 2006, 2012 and 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load landcover imagecollection\n",
    "landcover_collection = ee.ImageCollection('COPERNICUS/CORINE/V20/100m')\n",
    "\n",
    "# Load and the shapefile of the germany boundary\n",
    "gdf_germany = gpd.read_file(germany_boundary_shp_path)\n",
    "gdf_germany.crs = 'EPSG:25832'\n",
    "\n",
    "corine_file_paths = []\n",
    "years = [2000, 2006, 2012, 2018]\n",
    "for year in years:\n",
    "    file_path_corine_cropped = download_preprocess_corine(landcover_collection, year, gdf_germany)\n",
    "    corine_file_paths.append(file_path_corine_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all corine landcover datasets will be compared to produce a final layer giving constant constant forest classes for each pixel. More precisely forest class 311 and 312 will only be given when the pixel showed the respective class in all four years. Otherwise the pixel will be given the class 313, mixed forest type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load raster data into arrays\n",
    "arrays = []\n",
    "for file in corine_file_paths:\n",
    "    with rasterio.open(file) as src:\n",
    "        profile = src.profile\n",
    "        arrays.append(src.read(1))  # Assuming all rasters have one band\n",
    "\n",
    "# Convert list of arrays into a 3D numpy array (bands, rows, cols)\n",
    "stacked_arrays = np.stack(arrays)\n",
    "\n",
    "# Using all() along axis=0 ensures comparison across all years for each pixel\n",
    "corine_combined = np.where((stacked_arrays == stacked_arrays[0]).all(axis=0), stacked_arrays[0], 313)\n",
    "\n",
    "\n",
    "with rasterio.open(corine_combined_path, 'w', **profile) as dst:\n",
    "    dst.write(corine_combined.astype(profile['dtype']), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process Ndvi data and run seasonal Mann-Kendall test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \"_ndvi\" at the beginning of each filename\n",
    "for filename in os.listdir(ndvi_data_path):\n",
    "\n",
    "    new_filename = \"ndvi_\" + filename\n",
    "    os.rename(os.path.join(ndvi_data_path, filename), os.path.join(ndvi_data_path, new_filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Ndvi montly means numpy array and mask with corine landcover forest class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############  Load Ndvi Files ############\n",
    "\n",
    "# List all TIFF files in the directory\n",
    "ndvi_files = [f for f in os.listdir(ndvi_data_path) if f.endswith('.tif')]\n",
    "\n",
    "# Initialize an empty list to hold your arrays\n",
    "ndvi_arrays = []\n",
    "\n",
    "# Loop through the files, read each one, and append the array to your list\n",
    "for file in ndvi_files:\n",
    "    with rasterio.open(os.path.join(ndvi_data_path, file)) as src:\n",
    "        ndvi_arrays.append(src.read(1))  # read(1) reads the first band\n",
    "\n",
    "\n",
    "\n",
    "############  Stack and Reshape ndvi data ############\n",
    "        \n",
    "# Stack the arrays into a single 3D numpy array\n",
    "ndvi_stack = np.stack(ndvi_arrays, axis=0)\n",
    "\n",
    "# Ensure the time dimension is divisible by 3\n",
    "if ndvi_stack.shape[0] % 2 != 0:\n",
    "    print(\"Time dimension is not divisible by 2. Please adjust the array.\")\n",
    "else:\n",
    "    # Reshape the array to group every 3 timesteps\n",
    "    reshaped_data = ndvi_stack.reshape(-1, 2, ndvi_stack.shape[1], ndvi_stack.shape[2])\n",
    "    \n",
    "    # Calculate the mean across the new axis (1) corresponding to the grouped timesteps\n",
    "    ndvi_monthly_means = reshaped_data.mean(axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "############  mask NDVI data ############\n",
    "\n",
    "# Expand mask_2d to match array_3d's shape\n",
    "corine_mask_3d = corine_combined[None, :, :]  # Adds a new axis, making it (1, x, y)\n",
    "\n",
    "# Use broadcasting to apply the mask across all time steps\n",
    "ndvi_masked_monthly_means = np.where(np.isin(corine_mask_3d, [311,312,313]),\n",
    "                             ndvi_monthly_means,\n",
    "                             no_data_value)  # Replace non-masked values with np.nan or other desired value\n",
    "\n",
    "print(ndvi_masked_monthly_means.shape)\n",
    "\n",
    "del ndvi_stack, corine_combined, reshaped_data, ndvi_arrays, stacked_arrays, arrays "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run seasonal Man_kendall_test for each pixel and create raster output for significant trends (with direction) and slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run MK\n",
    "trend_result, trend_p_value, slope_result = perform_mk_test_progress(ndvi_masked_monthly_means, no_data_value, \"seasonal\")\n",
    "\n",
    "# use trend_p_value to create mask\n",
    "# Create a mask for significant trends (p < 0.025 or p>0.975)\n",
    "significant_trend_mask = np.logical_or(trend_p_value < 0.025, trend_p_value > 0.975)\n",
    "\n",
    "\n",
    "# safe results as tif\n",
    "create_trend_raster(template_file_ndvi, trend_result, output_path_trend_ndvi)\n",
    "create_trend_raster(template_file_ndvi, significant_trend_mask, output_path_sig_ndvi)\n",
    "create_trend_raster(template_file_ndvi, slope_result, output_path_slope_ndvi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Process Climate Data and run seasonal MK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next part DWD- montly gridded climate data was downloaded from the open climate data portal: https://opendata.dwd.de/climate_environment/CDC/grids_germany/monthly/. The data was downloaded and unzipped manually. Montly folders per variable were created containing the files for all years for the respective variable and month. This led to the following code for preprocessing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Function created to reproject, mask and safe to tif for all files for one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################\n",
    "# Function to reproject and mask a raster used for processing DWD Data\n",
    "########################################################\n",
    "\n",
    "def reproject_and_mask_rasters_dwd(data_folder, file_prefix, target_crs, corine_path, mask_crs_values=[311, 312, 313], source_crs = \"EPSG:31467\"):\n",
    "    \"\"\"\n",
    "    Reproject rasters to a target CRS and mask based on specified values.\n",
    "    \n",
    "    Parameters:\n",
    "    - base_dir: Directory containing the raster files.\n",
    "    - file_prefix: Prefix to identify specific raster files.\n",
    "    - target_crs: CRS to reproject the rasters to, e.g., 'EPSG:25832'.\n",
    "    - mask_crs_values: List of values in the mask raster to use for masking.\n",
    "    \"\"\"\n",
    "    month_folders = ['05_May', \"06_Jun\", \"07_Jul\", \"08_Aug\", \"09_Sep\"]\n",
    "    reprojected_file_paths = []  # Stores paths to the reprojected and masked TIFF files\n",
    "    \n",
    "    for month in month_folders:\n",
    "        month_number = month[:2]\n",
    "        month_dir = os.path.join(data_folder, month)\n",
    "        if os.path.isdir(month_dir):\n",
    "            for filename in os.listdir(month_dir):\n",
    "                if filename.startswith(f\"{file_prefix}_{month_number}\") and filename.endswith(\".asc\"):\n",
    "                    file_path = os.path.join(month_dir, filename)\n",
    "                    output_tiff_path = os.path.splitext(file_path)[0] + '_reprojected_masked.tif'\n",
    "                    \n",
    "                    # Load the raster data\n",
    "                    raster_data = rioxarray.open_rasterio(file_path, masked=True)\n",
    "                    raster_data.rio.set_crs(source_crs, inplace=True)\n",
    "\n",
    "                    # Reproject the raster to the target CRS\n",
    "                    reprojected_data = raster_data.rio.reproject(target_crs)\n",
    "\n",
    "                    # Assuming 'mask_raster_path' is the path to your mask raster\n",
    "                    # Load the mask raster and reproject to match the reprojected data\n",
    "                    mask_raster = rioxarray.open_rasterio(corine_path, masked=True)\n",
    "                    mask_reprojected = mask_raster.rio.reproject_match(reprojected_data, resampling=Resampling.nearest)\n",
    "\n",
    "                    # Apply mask based on specified values\n",
    "                    mask_condition = np.isin(mask_reprojected, mask_crs_values)\n",
    "                    masked_data = reprojected_data.where(mask_condition, np.nan)\n",
    "                        \n",
    "                    # Save the reprojected and masked raster\n",
    "                    masked_data.rio.to_raster(output_tiff_path)\n",
    "                    reprojected_file_paths.append(output_tiff_path)\n",
    "                    \n",
    "    return reprojected_file_paths\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# preprocessing for Temperature Data\n",
    "temp_tifs_path = reproject_and_mask_rasters_dwd(data_folder =temp_mean_data_path,\n",
    "                                            file_prefix = \"TAMM\", target_crs = \"EPSG:25832\", corine_path = corine_combined_path)\n",
    "\n",
    "# preprocessing for Precipitation Data\n",
    "precip_tifs_path = reproject_and_mask_rasters_dwd(data_folder =precip_data_path,\n",
    "                                            file_prefix = \"RSMS\", target_crs = \"EPSG:25832\", corine_path = corine_combined_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for DWD data processing and seasonal Mann-Kendall test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############  Function to create output paths ############\n",
    "def create_output_paths(variable, data_path, output_path):\n",
    "    variable_path = os.path.join(data_path, variable)\n",
    "\n",
    "    output_path_trend= os.path.join(output_path, variable, \"mk_trend_\" + variable + \".tif\")\n",
    "    output_path_slope = os.path.join(output_path, variable,\"mk_slope_\" + variable + \".tif\")\n",
    "    output_path_sig = os.path.join(output_path, variable,\"mk_sig_\" + variable + \".tif\")\n",
    "    \n",
    "    #print output path\n",
    "    return(output_path_trend, output_path_slope, output_path_sig, variable_path)\n",
    "\n",
    "############  Function to extract date from filename ############\n",
    "\n",
    "# option date_time_reverse is used to extract the date from the filename in the format \"YYYY_MM_DD\" instead of \"MM_YYYY\" which is used for the NDVI files\n",
    "def extract_date(filename, for_ndvi=False):\n",
    "    # Regular expression to find the date in the filename\n",
    "    \n",
    "    if for_ndvi:\n",
    "        match = re.search(r'(\\d{4})_(\\d{2})_(\\d{2})', filename)\n",
    "        if match:\n",
    "            # Return a tuple (Year, Month) for correct chronological sorting\n",
    "            return (match.group(1), match.group(2), match.group(3))\n",
    "        else:\n",
    "            return ('')\n",
    "\n",
    "    else:\n",
    "        match = re.search(r'(\\d{2})_(\\d{4})', filename)\n",
    "\n",
    "        if match:\n",
    "            # Return a tuple (Year, Month) for correct chronological sorting\n",
    "            return (match.group(2), match.group(1))\n",
    "        else:\n",
    "            return ('')\n",
    "\n",
    "\n",
    "############  Function to create data_stack ############\n",
    "\n",
    "def create_data_stack(variable_path, no_data_value = -9999): \n",
    "    # List all TIFF files in the directory\n",
    "    data_paths_unordered = [f for f in os.listdir(variable_path) if f.endswith('.tif')]\n",
    "\n",
    "\n",
    "    # Sorting the filenames using the extracted data\n",
    "    data_files = sorted(data_paths_unordered, key=lambda x: extract_date(x))\n",
    "\n",
    "    print(data_files[0])\n",
    "    print(data_files[1])\n",
    "\n",
    "    template_file = os.path.join(variable_path, data_files[0])\n",
    "\n",
    "    # Initialize an empty list to hold your arrays\n",
    "    clim_arrays = []\n",
    "\n",
    "    # Loop through the files, read each one, and append the array to your list\n",
    "    for file in data_files:\n",
    "        with rasterio.open(os.path.join(variable_path, file)) as src:\n",
    "            data_array = src.read(1)\n",
    "            data_array = np.nan_to_num(data_array, nan=no_data_value)\n",
    "            clim_arrays.append(data_array)\n",
    "\n",
    "    # Stack the arrays into a single 3D numpy array\n",
    "    clim_stack = np.stack(clim_arrays, axis=0)\n",
    "\n",
    "    print(clim_stack.shape)\n",
    "    return(clim_stack, template_file)\n",
    "  \n",
    "\n",
    "############  Main Function for processing and analysis for DWD climate Data ############\n",
    "\n",
    "def main(variable_number, data_path, output_path, no_data_value = -9999, variables = [\"temp_mean\",\"precip\"]):\n",
    "    \n",
    "    # variable\n",
    "    variable = variables[variable_number]\n",
    "    print(variable)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # create output paths\n",
    "    print(\"Create output paths\")\n",
    "    output_path_trend, output_path_slope, output_path_sig, variable_path = create_output_paths(variable, data_path, output_path)\n",
    "    \n",
    "    # print output paths\n",
    "    print(\"Output paths: \", output_path_trend, output_path_slope, output_path_sig)\n",
    "\n",
    "    # create data stack\n",
    "    print(\"Create data stack\")\n",
    "    data_stack, template_file = create_data_stack(variable_path, no_data_value = no_data_value)\n",
    "    \n",
    "    # print data stack\n",
    "    print(\"Data stack: \", data_stack.shape)\n",
    "    print(\"Template file: \", template_file)\n",
    "\n",
    "    # perform mk test\n",
    "    print(\"Perform MK test\")\n",
    "    trend_result, trend_p_value, slope_result = perform_mk_test_progress(data_stack, no_data_value, test = \"seasonal\")\n",
    "    significant_trend_mask = np.logical_or(trend_p_value < 0.025, trend_p_value > 0.975)\n",
    "\n",
    "\n",
    "    # create output raster\n",
    "    print(\"Create output raster\")\n",
    "    create_trend_raster(template_file, trend_result, output_path_trend)\n",
    "    create_trend_raster(template_file, slope_result, output_path_slope)\n",
    "    create_trend_raster(template_file, significant_trend_mask, output_path_sig)\n",
    "\n",
    "    print(\"done\")\n",
    "    return output_path_trend, output_path_slope, output_path_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining variables and Executing function for all three climate variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For this step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable folders\n",
    "dwd_variables = [\"temp_mean\",\"precip\",]\n",
    "\n",
    "# For temperature\n",
    "output_path_trend_temp, output_path_slope_temp, output_path_sig_temp = main(0, data_path, output_path_climate_mk, no_data_value = -9999, variables = dwd_variables)\n",
    "\n",
    "# For precipitation\n",
    "output_path_trend_precip, output_path_slope_precip, output_path_sig_precip = main(1, data_path, output_path_climate_mk, no_data_value = -9999, variables = dwd_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Correlation Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the correlation analysis we will look at 6 german forest areas. For each forest area a mean annual NDVI time series will be correlated with mean annual time series of our three climatic variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now start working with xarray datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############  Function to create xarray dataarrayfrom files and reshape to annual means ############\n",
    "\n",
    "def create_data_stack_xarray(variable, no_data_value=-9999, for_ndvi=False, show_months = False, ndvi_slope_path = output_path_slope_ndvi):\n",
    "    variable_path = os.path.join(data_path, variable)\n",
    "\n",
    "    # List all TIFF files in the directory\n",
    "    data_paths_unordered = [f for f in os.listdir(variable_path) if f.endswith('.tif')]\n",
    "\n",
    "    # ndvi slope to use only negative values\n",
    "    if for_ndvi:\n",
    "        ndvi_slope = rioxarray.open_rasterio(ndvi_slope_path)\n",
    "\n",
    "    # Sorting the filenames using the extracted data\n",
    "    data_files = sorted(data_paths_unordered, key=lambda x: extract_date(x, for_ndvi))\n",
    "\n",
    "    # Initialize an empty list to hold your DataArrays\n",
    "    clim_dataarrays = []\n",
    "    \n",
    "    # Initialize a list to hold the time coordinates\n",
    "    time_coords = []\n",
    "\n",
    "    # Loop through the files, open each one as a DataArray, and append to the list\n",
    "    for file in data_files:\n",
    "        da = rioxarray.open_rasterio(os.path.join(variable_path, file))\n",
    "        # Replace no data values (-9999) with NaN directly\n",
    "        da = da.where(da != no_data_value, np.nan)\n",
    "        clim_dataarrays.append(da)\n",
    "        \n",
    "        # Extract date from filename and convert to datetime\n",
    "        date_info = extract_date(file, for_ndvi)\n",
    "        if for_ndvi:\n",
    "            year, month, day = date_info\n",
    "        else:\n",
    "            year, month = date_info\n",
    "            day = 1\n",
    "        time_coords.append(pd.Timestamp(year=int(year), month=int(month), day=int(day)))\n",
    "\n",
    "    # Concatenate all DataArrays along a new dimension called 'time'\n",
    "    clim_stack_xarray = xr.concat(clim_dataarrays, dim='time')\n",
    "    \n",
    "    # Set the time coordinate\n",
    "    clim_stack_xarray = clim_stack_xarray.assign_coords(time=('time', time_coords))\n",
    "\n",
    "    if for_ndvi:\n",
    "        corine_combined = rioxarray.open_rasterio(corine_combined_path)\n",
    "        land_cover_mask = corine_combined.isin([311, 312, 313])\n",
    "        land_cover_mask_3d = land_cover_mask.broadcast_like(clim_stack_xarray.isel(time=0))\n",
    "        # Apply land cover mask and replace values outside the mask with np.nan\n",
    "        clim_stack_xarray = xr.where(land_cover_mask_3d, clim_stack_xarray, np.nan)\n",
    "        clim_stack_xarray = clim_stack_xarray.where(ndvi_slope < 0, np.nan)\n",
    "\n",
    "    # Group by year and list months for each year\n",
    "    grouped_by_year = clim_stack_xarray.groupby('time.year')\n",
    "\n",
    "    if show_months:\n",
    "        for year, group in grouped_by_year:\n",
    "            months = group.time.dt.month.values\n",
    "            print(f\"Year: {year}, Months: {months}\")\n",
    "\n",
    "\n",
    "    # Calculate annual means, ensuring no_data_value is correctly handled as np.nan\n",
    "    clim_stack_xarray_annual_means = clim_stack_xarray.resample(time='AS').mean(skipna=True)\n",
    "    clim_stack_xarray_annual_means.name = variable\n",
    "\n",
    "    return clim_stack_xarray_annual_means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create Xarrays datasets and reshape to annual means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_mean_xr = create_data_stack_xarray(variable = \"temp_mean\", no_data_value = -9999)\n",
    "precip_xr = create_data_stack_xarray(variable = \"precip\", no_data_value = -9999)\n",
    "ndvi_xr = create_data_stack_xarray(variable = \"ndvi_seasonal\", no_data_value = -9999, for_ndvi= True, show_months=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Load and process forest area shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gebiete = [[\"Sauerland-Rothaargebirge\",\"Arnsberger Wald\", \"Bergisches Land\",\"Lahn-Dill-Bergland\"],\n",
    "           [\"Harz/Sachsen-Anhalt\", \"Südharz\",\"Harz/Niedersachsen\",\"Harz Sachsen-Anhalt (Mansfelder Land)\"],\n",
    "           [\"Südschwarzwald\"],\n",
    "           [\"Thüringer Schiefergebirge/Obere Saale\",\"Thüringer Wald\",\"Frankenwald\"]]\n",
    "\n",
    "\n",
    "# load naturparke with gpd\n",
    "gdf_naturparke = gpd.read_file(naturparke_shp_path)\n",
    "\n",
    "# transform to epsg 25832\n",
    "gdf_naturparke = gdf_naturparke.to_crs(\"EPSG:25832\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the merged polygons\n",
    "merged_polygons = []\n",
    "\n",
    "for gemeinde_names in gebiete:\n",
    "    # Subset the GeoDataFrame for each name in the sublist\n",
    "    subset_gdf = gdf_naturparke[gdf_naturparke['NAME'].isin(gemeinde_names)]\n",
    "    \n",
    "    # Merge the polygons found in the subset if there's any, otherwise skip\n",
    "    if not subset_gdf.empty:\n",
    "        merged_polygon = subset_gdf.unary_union\n",
    "        # Store the merged polygon with a reference to the gemeinde names for identification\n",
    "        merged_polygons.append((gemeinde_names, merged_polygon))\n",
    "\n",
    "# To convert the list of merged polygons back into a GeoDataFrame\n",
    "# First, create a list of dictionaries with the appropriate structure\n",
    "data = [{'NAME': names, 'geometry': polygon} for names, polygon in merged_polygons]\n",
    "# Then, convert this list into a GeoDataFrame\n",
    "merged_gdf_naturparke = gpd.GeoDataFrame(data, crs=gdf_naturparke.crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf_naturparke.NAME = [\"Rothaargebirge\",\"Harz\",\"Schwarzwald_sued_mitte\",\"Thüringer_Schiefergebirge\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Calculate means over polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_mean_over_time(xarrays, gdf, corine_combined_path):\n",
    "    \"\"\"\n",
    "    Calculates mean values for each xarray.DataArray within polygons defined in a GeoDataFrame for each timestep,\n",
    "    with keys in the output dictionary being the 'NAME' of the polygons.\n",
    "\n",
    "    Parameters:\n",
    "    - xarrays: List of xarray.DataArray objects. Each DataArray should have a 'time' dimension.\n",
    "    - gdf: A GeoDataFrame containing polygons, each with a 'NAME' attribute.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary with keys as polygon names from the GeoDataFrame 'NAME' column and values as Pandas DataFrames.\n",
    "      Each DataFrame contains mean values for each timestep as rows and each DataArray as columns.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for _, polygon in gdf.iterrows():\n",
    "        polygon_name = polygon['NAME']\n",
    "        # Initialize a DataFrame to hold results for the current polygon by its name\n",
    "        polygon_means = {da.name: [] for da in xarrays}  # Assuming each DataArray has a 'name' attribute\n",
    "        \n",
    "\n",
    "        for da in xarrays:\n",
    "\n",
    "       \n",
    "            da.rio.write_crs(\"EPSG:25832\", inplace=True)  # Example with EPSG:4326, adjust CRS as neede\n",
    "\n",
    "\n",
    "            # Clip the DataArray by the current polygon\n",
    "            clipped = da.rio.clip([polygon.geometry], crs=gdf.crs, drop=True, all_touched=True)\n",
    "            # Calculate the mean for each time step and store it\n",
    "            for time in clipped.time:\n",
    "                mean_val = clipped.sel(time=time).mean(skipna=True).values.item()\n",
    "                polygon_means[da.name].append(mean_val)\n",
    "   \n",
    "        # Convert the lists of means into a DataFrame with a time index\n",
    "        index = pd.to_datetime([str(time.values)[:10] for time in clipped.time])  # Adjust string slicing as necessary\n",
    "        results[polygon_name] = pd.DataFrame(polygon_means, index=index)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xarrays = [temp_mean_xr, precip_xr, ndvi_xr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_ndvi_annual_series= polygon_mean_over_time(xarrays, merged_gdf_naturparke, corine_combined_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_ndvi_annual_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Analyze correlations between ndvi and climate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_correlations_and_significance_table_with_lag(results_dict, lag=0):\n",
    "    \"\"\"\n",
    "    Calculates the correlation and its significance of 'ndvi_seasonal' with the other three DataArrays for each polygon,\n",
    "    including the option to add a lag.\n",
    "\n",
    "    Parameters:\n",
    "    - results_dict: The output from the `polygon_mean_over_time` function.\n",
    "    - lag: The lag to apply to 'ndvi_seasonal' before correlation calculation.\n",
    "\n",
    "    Returns:\n",
    "    - A Pandas DataFrame with polygon names as columns, rows for correlations and p-values for each DataArray comparison, considering the specified lag.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    data = {\"Metric\": []}\n",
    "    polygons = list(results_dict.keys())\n",
    "    for polygon_name in polygons:\n",
    "        data[polygon_name] = []\n",
    "    \n",
    "    for column in next(iter(results_dict.values())).columns:\n",
    "        \n",
    "        if column != 'ndvi_seasonal':  # Skip 'ndvi_seasonal' itself\n",
    "            data[\"Metric\"].extend([f\"Correlation with {column} (lag={lag})\", f\"P-value with {column} (lag={lag})\"])\n",
    "            for polygon_name, df in results_dict.items():\n",
    "                # Apply lag\n",
    "                ndvi_seasonal_lagged = df['ndvi_seasonal'].shift(lag)\n",
    "                # Calculate correlation and p-value with lag\n",
    "                corr, p_value = pearsonr(ndvi_seasonal_lagged.dropna(), df[column][lag:].dropna())\n",
    "                data[polygon_name].extend([corr, p_value])\n",
    "                \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    results_df = pd.DataFrame(data).set_index(\"Metric\")\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_ndvi_annual_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `results` is your output from `polygon_mean_over_time`\n",
    "correlations_0 = calculate_correlations_and_significance_table_with_lag(climate_ndvi_annual_series, lag =0)\n",
    "\n",
    "correlations_1 = calculate_correlations_and_significance_table_with_lag(climate_ndvi_annual_series, lag =1)\n",
    "\n",
    "correlations_2 = calculate_correlations_and_significance_table_with_lag(climate_ndvi_annual_series, lag =2)\n",
    "\n",
    "correlations_3 = calculate_correlations_and_significance_table_with_lag(climate_ndvi_annual_series, lag =3)\n",
    "\n",
    "\n",
    "\n",
    "# Now `correlations` contains the correlation of 'ndvi_seasonal' with each of the other arrays for each polygon\n",
    "\n",
    "# give mean values over rows\n",
    "\n",
    "print(\"Lag 0:\",correlations_1.mean(axis=1),\"\\n\")\n",
    "print(\"Lag 1:\",correlations_1.mean(axis=1),\"\\n\")\n",
    "print(\"Lag 2:\",correlations_2.mean(axis=1),\"\\n\")\n",
    "print(\"Lag 3:\",correlations_3.mean(axis=1),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Layers with trend direction and signifcance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ndvi, temperature and precipitation produce a layer with information on trend direction and significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def slope_trend_to_sig_trend(output_path_slope, output_path_trend, no_data_value=-9999):\n",
    "    # Load the slope and trend raster datasets with masking enabled for no_data values\n",
    "    slope = rioxarray.open_rasterio(output_path_slope, masked=True)\n",
    "    trend = rioxarray.open_rasterio(output_path_trend, masked=True)\n",
    "\n",
    "    # Create a new xarray DataArray for significant trends initialized with the no_data_value\n",
    "  # Initialize the output dataset with the same shape and coordinates as the input, filled with no_data_value\n",
    "    sig_trends = xr.full_like(slope, no_data_value)\n",
    "\n",
    "    # Apply conditions to determine significant trends\n",
    "    # Positive slope and trend == 1\n",
    "    sig_trends = xr.where((slope > 0) & (trend == 1), 2, sig_trends)\n",
    "    # Positive slope and trend == 0\n",
    "    sig_trends = xr.where((slope > 0) & (trend == 0), 1, sig_trends)\n",
    "    # Negative slope and trend == -1\n",
    "    sig_trends = xr.where((slope < 0) & (trend == -1), -2, sig_trends)\n",
    "    # Negative slope and trend == 0\n",
    "    sig_trends = xr.where((slope < 0) & (trend == 0), -1, sig_trends)\n",
    "\n",
    "    sig_trends.rio.write_crs(trend.rio.crs, inplace=True)\n",
    "    sig_trends.rio.set_nodata(no_data_value, inplace=True)\n",
    "\n",
    "    # Save the significant trends raster to a new file\n",
    "    # The output path is derived from the slope file path to avoid overwriting original data\n",
    "    output_path_sig_trends = output_path_slope.replace(\"slope\", \"combined_trend_slope\")\n",
    "    sig_trends.rio.to_raster(output_path_sig_trends)\n",
    "\n",
    "    # Print the path of the saved significant trends raster for verification\n",
    "    return output_path_sig_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_sig_trends_ndvi = slope_trend_to_sig_trend(output_path_slope_ndvi, output_path_trend_ndvi, no_data_value = -9999)\n",
    "\n",
    "output_path_sig_trends_temp = slope_trend_to_sig_trend(output_path_slope_temp, output_path_trend_temp, no_data_value = -9999)\n",
    "\n",
    "output_path_sig_trends_precip = slope_trend_to_sig_trend(output_path_slope_precip, output_path_trend_precip, no_data_value = -9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Analyzing ndvi Trend distribution over forest classes per forest area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corine_combined = rioxarray.open_rasterio(corine_combined_path, masked=True)\n",
    "trend_ndvi = rioxarray.open_rasterio(output_path_trend_ndvi, masked=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_trend_statistics(corine_combined, sig_trend_path, polygon = None, germany=False, forest_classes=[311, 312, 313]):\n",
    "    sig_trend = rioxarray.open_rasterio(sig_trend_path, masked=True)\n",
    "\n",
    "    if germany:\n",
    "        corine_combined_crop = corine_combined\n",
    "        sig_trend_crop = sig_trend\n",
    "    else:   \n",
    "        print(polygon[\"NAME\"])\n",
    "        corine_combined_crop = corine_combined.rio.clip([polygon.geometry], \"EPSG:25832\", drop=True, all_touched=True)\n",
    "        sig_trend_crop = sig_trend.rio.clip([polygon.geometry], \"EPSG:25832\", drop=True, all_touched=True)\n",
    "\n",
    "    print(np.unique(sig_trend_crop))\n",
    "\n",
    "    forest_class_list = []\n",
    "    positive_trend_percentage_list = []\n",
    "    negative_trend_percentage_list = []\n",
    "    sig_positive_trend_percentage_list = []\n",
    "    sig_negative_trend_percentage_list = []\n",
    "\n",
    "    for land_cover_class in forest_classes:\n",
    "        class_mask = (corine_combined_crop == land_cover_class)\n",
    "\n",
    "        positive_trend_count = np.sum((sig_trend_crop == 1) & class_mask)\n",
    "        sig_positive_trend_count = np.sum((sig_trend_crop == 2) & class_mask)\n",
    "        \n",
    "        negative_trend_count = np.sum((sig_trend_crop == -1) & class_mask)\n",
    "        sig_negative_trend_count = np.sum((sig_trend_crop == -2) & class_mask)\n",
    "\n",
    "        total_pixels = np.sum(class_mask)\n",
    "\n",
    "        # Calculate percentages and directly convert to scalar values for appending\n",
    "        positive_trend_percentage = (positive_trend_count / total_pixels).item() if total_pixels else np.nan\n",
    "        sig_positive_trend_percentage = (sig_positive_trend_count / total_pixels).item() if total_pixels else np.nan\n",
    "\n",
    "        negative_trend_percentage = (negative_trend_count / total_pixels).item() if total_pixels else np.nan\n",
    "        sig_negative_trend_percentage = (sig_negative_trend_count / total_pixels).item() if total_pixels else np.nan\n",
    "\n",
    "        forest_class_list.append(land_cover_class)\n",
    "        positive_trend_percentage_list.append(np.round(positive_trend_percentage * 100, 1))\n",
    "        negative_trend_percentage_list.append(np.round(negative_trend_percentage * 100, 1))\n",
    "\n",
    "        sig_positive_trend_percentage_list.append(np.round(sig_positive_trend_percentage * 100, 1))\n",
    "        sig_negative_trend_percentage_list.append(np.round(sig_negative_trend_percentage * 100, 1))\n",
    "\n",
    "    # Create the DataFrame with correct column assignments\n",
    "    results_df = pd.DataFrame({\n",
    "        'Forest Class': forest_class_list,\n",
    "        'Positive Trend Percentage': positive_trend_percentage_list,\n",
    "        'Negative Trend Percentage': negative_trend_percentage_list,\n",
    "        'Significant Positive Trend Percentage': sig_positive_trend_percentage_list,\n",
    "        'Significant Negative Trend Percentage': sig_negative_trend_percentage_list\n",
    "    })\n",
    "\n",
    "    return results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame for the aggregated results\n",
    "aggregated_results = pd.DataFrame()\n",
    "\n",
    "\n",
    "############  Calculate trend statistics for germany ############\n",
    "\n",
    "germany_results = calculate_trend_statistics(corine_combined, output_path_sig_trends_ndvi, germany=True)\n",
    "germany_results['Area'] = \"Germany\"\n",
    "aggregated_results = pd.concat([aggregated_results, germany_results], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "############  Calculate Trend Statistics for forest areas ############\n",
    "\n",
    "# Iterate over each polygon\n",
    "for index, polygon in merged_gdf_naturparke.iterrows():\n",
    "    # Calculate trend statistics for the current polygon\n",
    "    current_results = calculate_trend_statistics(corine_combined, output_path_sig_trends_ndvi, polygon)\n",
    "    \n",
    "    # Insert polygon name as a column for identification\n",
    "    current_results['Area'] = polygon[\"NAME\"]\n",
    "    \n",
    "    # Append the results to the aggregated DataFrame\n",
    "    aggregated_results = pd.concat([aggregated_results, current_results], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# Set a new index based on 'Polygon Name' and 'Forest Class'\n",
    "aggregated_results.set_index(['Area', 'Forest Class'], inplace=True)\n",
    "\n",
    "aggregated_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Plotting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############  Preparing Data ############\n",
    "\n",
    "\n",
    "gdf_germany = gpd.read_file(germany_boundary_shp_path)\n",
    "gdf_germany.crs = 'EPSG:25832'\n",
    "polygon_gdf = gpd.GeoDataFrame(merged_gdf_naturparke, geometry='geometry', crs=merged_gdf_naturparke.crs)\n",
    "\n",
    "corine_combined_clean = corine_combined.where(corine_combined.isin([311, 312, 313]), np.nan)\n",
    "\n",
    "ndvi_trend_slope_combined = rioxarray.open_rasterio(output_path_sig_trends_ndvi)\n",
    "temp_trend_slope_combined = rioxarray.open_rasterio(output_path_sig_trends_temp)\n",
    "precip_trend_slope_combined = rioxarray.open_rasterio(output_path_sig_trends_precip)\n",
    "\n",
    "ndvi_trend_slope_combined = ndvi_trend_slope_combined.where(ndvi_trend_slope_combined != no_data_value)\n",
    "temp_trend_slope_combined = temp_trend_slope_combined.where(temp_trend_slope_combined != no_data_value)\n",
    "precip_trend_slope_combined = precip_trend_slope_combined.where(precip_trend_slope_combined != no_data_value)\n",
    "\n",
    "\n",
    "ndvi_slope = rioxarray.open_rasterio(output_path_slope_ndvi)\n",
    "temp_slope = rioxarray.open_rasterio(output_path_slope_temp)\n",
    "precip_slope = rioxarray.open_rasterio(output_path_slope_precip)\n",
    "\n",
    "ndvi_slope = ndvi_slope.where(ndvi_slope != no_data_value)\n",
    "temp_slope = temp_slope.where(temp_slope != no_data_value)\n",
    "precip_slope = precip_slope.where(precip_slope != no_data_value)\n",
    "\n",
    "\n",
    "############  Preparing colors and legend entries etc. ############\n",
    "\n",
    "# creating a arrays containing the relevant legend labels\n",
    "legend_5_labels = [\"sig. negative\", \"negative (-)\", \"neutral\", \"positive (+)\", \"sig. positive\"]\n",
    "legend_slope = [\"neg (-)\",\"neutral\",\" pos (+)\"]\n",
    "\n",
    "# defining all the needed colorlists\n",
    "color_list_temp = [\"#bd0026\", \"#f03b20\", \"#fd8d3c\", \"#fecc5c\", \"#ffffb2\"]\n",
    "color_list_plasma = [\"#0d0887\", \"#7e03a8\", \"#cc4778\", \"#f89540\", \"#f0f921\"]\n",
    "color_map_plasma =  ListedColormap(color_list_plasma)\n",
    "color_map_plasmaSeg = LinearSegmentedColormap.from_list(\"color_list_temp\", color_list_temp)\n",
    "\n",
    "color_list_ndvi = [\"#d01c8b\", \"#f1b6da\", \"#f7f7f7\", \"#b8e186\", \"#4dac26\"]\n",
    "color_map_ndvi_slope_trend = ListedColormap(color_list_ndvi)\n",
    "color_map_ndvi = LinearSegmentedColormap.from_list(\"color_list_ndvi\", color_list_ndvi)\n",
    "\n",
    "color_list_precip = [\"#a6611a\", \"#dfc27d\", \"#f5f5f5\", \"#80cdc1\", \"#018571\"]\n",
    "color_map_precip = LinearSegmentedColormap.from_list(\"color_list_precip\", color_list_precip)\n",
    "color_map_precip_slope_trend =  ListedColormap(color_list_precip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### plotting fuction WITH flexible legend labels ###########################################\n",
    "\n",
    "\n",
    "# setting up the function, including geodata, boundaries and more for the map\n",
    "def plot_data_flex(dataset, cmap, vmin, vmax, plot_polygons=False, map_title='', legend_labels=None):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Plot Germany with gray fill and black boundary\n",
    "    gdf_germany.plot(ax=ax, color='lightgray', edgecolor='black', alpha=1, linewidth=1.5)\n",
    "\n",
    "    # Plot rasterio dataset\n",
    "    dataset.plot(ax=ax, alpha=1, vmin=vmin, vmax=vmax, cmap=cmap, add_colorbar=False)\n",
    "\n",
    "    if plot_polygons:\n",
    "        # Plot polygon boundaries\n",
    "        polygon_gdf.boundary.plot(ax=ax, color='black', linewidth=1, linestyle='dashdot')\n",
    "\n",
    "    # Remove x and y axis ticks and labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "    # Set the map title if specified\n",
    "    if map_title:\n",
    "        ax.set_title(map_title, fontweight='bold') # Title in bold\n",
    "        # larger title\n",
    "        ax.title.set_fontsize(13)\n",
    "\n",
    "    # Remove the border around the map\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    # Create a colorbar\n",
    "    cbar = fig.colorbar(plt.cm.ScalarMappable(norm=Normalize(vmin=vmin, vmax=vmax), cmap=cmap),\n",
    "                        ax=ax, fraction=0.02, pad=0.04)\n",
    "    \n",
    "    # Adjust the colorbar to work with both groups of 3 and 5 legend labels\n",
    "    if legend_labels:\n",
    "        # Calculate evenly spaced ticks between vmin and vmax based on the number of labels\n",
    "        ticks = np.linspace(vmin, vmax, len(legend_labels))\n",
    "        cbar.set_ticks(ticks)\n",
    "        cbar.ax.set_yticklabels(legend_labels)  # Set custom legend labels\n",
    "    else:\n",
    "        # Default case for 3 labels if legend_labels is not specified\n",
    "        cbar.set_ticks([vmin, (vmin+vmax)/2, vmax])\n",
    "        cbar.ax.set_yticklabels([str(vmin), str((vmin+vmax)/2), str(vmax)])  # Default numeric labels\n",
    "    \n",
    "    # Adding a north arrow\n",
    "    ax.annotate('N', xy=(1.1,0.99), xytext=(1.1, 0.9),\n",
    "                arrowprops=dict(facecolor='black', width=5, headwidth=15),\n",
    "                xycoords='axes fraction', textcoords='axes fraction',\n",
    "                fontsize=20, ha='center')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting CLC forest classes and our designated AOIs\n",
    "plot_data_flex(corine_combined_clean, ListedColormap(['#66a61e', '#1b9e77', '#7570b3']), 311, 313, \n",
    "    plot_polygons = True,\n",
    "    map_title=' ', #'Combined CLC Forest Classes and AOIs'\n",
    "    legend_labels=['Decidious Forest', 'Coniferous Forest', 'Mixed Forest'])\n",
    "\n",
    "\n",
    "\n",
    "#################SLOPE###################\n",
    "#slope of ndvi\n",
    "plot_data_flex(dataset = ndvi_slope, cmap= color_map_ndvi, vmin= -0.002, vmax = 0.002, \n",
    "    plot_polygons=True, \n",
    "    map_title=' ',#'Slope of NDVI'\n",
    "    legend_labels=legend_slope)\n",
    "\n",
    "#slope of temperature\n",
    "plot_data_flex(dataset = temp_slope, cmap = 'plasma', vmin= -1, vmax = 1, \n",
    "    plot_polygons=True,\n",
    "    map_title=' ',#'Slope of temperature'\n",
    "    legend_labels=legend_slope)\n",
    "\n",
    "#slope of precipitation\n",
    "plot_data_flex(dataset = precip_slope, cmap= color_map_precip, vmin= -1, vmax = 1,\n",
    "    plot_polygons=True,\n",
    "    map_title=' ' ,#'Slope of precipitation'\n",
    "    legend_labels=legend_slope)\n",
    "\n",
    "\n",
    "##########TREND DIR AND SIGN###############\n",
    "# for ndvi\n",
    "plot_data_flex(dataset = ndvi_trend_slope_combined, cmap= color_map_ndvi_slope_trend, \n",
    "    vmin= -2, vmax = 2, plot_polygons=True, map_title=' ' #'Combined slope and trend for NDVI'\n",
    "    , legend_labels=legend_5_labels)\n",
    "\n",
    "#for temperature\n",
    "plot_data_flex(dataset = temp_trend_slope_combined, cmap= color_map_plasma \n",
    "    , vmin= -1, vmax = 2\n",
    "    , plot_polygons=True, map_title=' '#'Combined slope and trend for temperature'\n",
    "    , legend_labels=legend_5_labels)\n",
    "\n",
    "# for precipitation\n",
    "plot_data_flex(dataset = precip_trend_slope_combined, cmap= color_map_precip_slope_trend, \n",
    "    vmin= -1.7, vmax = 1.7, plot_polygons=True, map_title= ' ' #'Combined slope and trend for precipitation'\n",
    "    , legend_labels=legend_5_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
